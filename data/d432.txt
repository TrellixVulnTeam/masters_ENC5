If there's one thing that tech companies, retailers, content creators and investors can agree on, it's this: There's plenty of money to be made from the metaverse. But as CEOs try to elbow past their rivals to gain a foothold in the still nascent digital space, some psychologists and mental health experts say the race to turn a profit is taking attention away from a crucial question: Will the metaverse be a safe place, especially for kids and teens? The answer isn't encouraging. Recent research has shown myriad negative effects of social media on the psyches of children and adolescents, from the prevalence of bullying and harassment to self-esteem and body image issues. Those same pitfalls could be just as prevalent — if not worse — in the wide-open metaverse, with its series of vast virtual worlds intended for both work and play. But if tech companies take those concerns seriously from the beginning, and build solutions into their metaverse products, they could actually benefit children's mental health, some experts say. "All of these new tools, and all of these new possibilities, could be used for good or for evil," Mitch Prinstein, a clinical psychologist who serves as chief science officer for the American Psychological Association, tells CNBC Make It.

Today's social media platforms are already dangerous for some kids and teens. Virtual reality's level of immersion could make those problems even worse, says Albert "Skip" Rizzo, a psychologist who serves as the director for medical virtual reality at USC's Institute for Creative Technologies. "There's a potency about being immersed in a world that is different than observing and interacting…through a flat screen monitor," Rizzo says. "Once you're actually embodied in a space, even though you can't be physically touched, we can be exposed to things that take on a level of realism that could be psychologically assaulting."

The use of 3D digital avatars in the metaverse carries another problem, too: Being able to modify your likeness to project a version of yourself that differs from real life can be "pretty dangerous for adolescents, in particular," Prinstein says. "You are what other people think about you in adolescence," he says. "And the idea of being able to fictionalize your identity and receive very different feedback can really mess with a teenager's identity."

Prinstein worries that tech companies are targeting their social media and metaverse platforms at this highly suggestible demographic — during an important stretch of their brains' mental and emotional development — with potentially dire consequences. "This is just an exacerbation of the problems that we've already started to see with the effects of social media," he says. "This is creating more loneliness. This is creating far more body image concerns [and] exposure to dangerous content that's related to suicidality."

Some problems are already here

In December, Meta launched a virtual reality social platform, Horizon Worlds. Last March, Microsoft launched a cloud collaboration service for virtual 3D business meetings. Other companies, like Roblox and Epic Games, are grabbing toeholds in the metaverse through popular online games. One such game publisher, VRChat, already shows evidence of dangers for young users. In December, research from the nonprofit Center for Countering Digital Hate (CCDH) found that minors were regularly exposed to graphic sexual content, racist and violent language, bullying and other forms of harassment on VRChat's platform, which is typically accessed through Meta's Oculus headsets.

Meta and Oculus have policies prohibiting these sorts of negative behaviors on their VR platforms. When reached for comment, a Meta spokesperson referred CNBC Make It to the company's previous statements on trying to build a metaverse "responsibly," and the Oculus platform's tools for reporting abuse and blocking other users. VRChat did not immediately respond to CNBC Make It's request for comment. That's part of the problem, says CCDH CEO Imran Ahmed: Safety policies, however well-intentioned, can be difficult to monitor and enforce in virtual spaces. "Virtual reality really does need a lot of safety built in from the start, because you can't search [the metaverse] for hate or sexual abuse," he says. "You can't. It happens in an instant [and] there's nothing you can do." Ahmed's prediction: Parents will need to be wary about their kids' access to the metaverse. "I think parents will be asking themselves: Do I feel safe knowing that Mark Zuckerberg is the guy in charge of deciding who influences my children, who might be able to bully them, and whether or not they're safe in cyberspace?" he says.

The irony is that virtual reality and the metaverse have massive promise for improving users' mental health. Rizzo's research at USC, for example, shows potential for virtual reality treatments to promote empathy in patients and help with issues like psychological trauma and PTSD. But Rizzo and Prinstein agree the onus is on tech companies to prioritize the safety of their users over their own incentive to turn a profit. Ahmed says tech companies could employ tools to ensure the metaverse's safety for young users, including strict age verification tools to prevent predators from posing as younger users, plentiful content moderators and "rapid response" when users report violations of inappropriate behavior. "There's no reason why there couldn't be the presence of moderators in spaces in which children are present [or] virtual chaperones," he says. "But, of course, that would require money."